lkx@lkx-virtual-machine:~/code$ python3 train.py config/train_wikitext.py
Overriding config with config/train_wikitext.py:
out_dir = 'out-wikitext'
eval_interval = 250                 # 每250次迭代进行一次评估
eval_iters = 20                     # 减少计算量，评估时使用20个批次（原为200）
log_interval = 50                   # 减少Log，原为10

always_save_checkpoint = False

wandb_log = False 
wandb_project = 'wikitext_large'
wandb_run_name = 'mini-gpt'

dataset = 'wikitext_large'
gradient_accumulation_steps = 4
batch_size = 12                     # 减少内存占用，调整每批次样本数（原为16）
block_size = 64                     # 缩短序列长度（原为256）

n_layer = 4                         # 减少模型规模（原为8）
n_head = 4                          # 减小注意力头数（原为8）
n_embd = 128                        # 减小嵌入维度（原为512）
dropout = 0.2

learning_rate = 1e-3 
max_iters = 20000
lr_decay_iters = 20000
min_lr = 1e-4 
beta2 = 0.99 

warmup_iters = 100 


tokens per iteration will be: 3,072
Initializing a new model from scratch
number of parameters: 7.23M
/home/lkx/code/train.py:122: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))
/home/lkx/.local/lib/python3.10/site-packages/torch/amp/grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.
  warnings.warn(
num decayed parameter tensors: 17, with 7,225,344 parameters
num non-decayed parameter tensors: 34, with 6,912 parameters
using fused AdamW: False
compiling the model... (takes a ~minute)
step 0: train loss 10.8504, val loss 10.8599
iter 50: loss 8.9801, time 1184.47ms
iter 100: loss 7.2301, time 1222.88ms
iter 150: loss 7.1823, time 1213.40ms
iter 200: loss 6.4333, time 1211.28ms
step 250: train loss 6.3891, val loss 5.6767
saving checkpoint to out-wikitext
iter 250: loss 6.4225, time 4746.63ms
iter 300: loss 6.5150, time 1223.23ms
iter 350: loss 6.1813, time 1222.92ms
iter 400: loss 6.1261, time 1222.26ms
iter 450: loss 6.1599, time 1229.15ms
step 500: train loss 6.1014, val loss 5.3214
saving checkpoint to out-wikitext
iter 500: loss 5.8694, time 4799.89ms
iter 550: loss 6.0147, time 1220.06ms
iter 600: loss 6.0467, time 1217.32ms
iter 650: loss 5.9329, time 1215.06ms
iter 700: loss 6.0858, time 1215.96ms
step 750: train loss 5.8605, val loss 5.2769
saving checkpoint to out-wikitext
iter 750: loss 5.8889, time 4807.51ms
iter 800: loss 5.8986, time 1218.35ms
iter 850: loss 5.9189, time 1211.46ms
iter 900: loss 5.8844, time 1230.70ms
iter 950: loss 6.0347, time 1229.66ms
step 1000: train loss 5.7009, val loss 5.1568
saving checkpoint to out-wikitext
iter 1000: loss 5.6184, time 6954.74ms
iter 1050: loss 6.1104, time 1219.98ms
iter 1100: loss 5.4952, time 1222.83ms
iter 1150: loss 5.5633, time 1222.15ms
iter 1200: loss 5.4085, time 1221.53ms
step 1250: train loss 5.6182, val loss 5.1370
saving checkpoint to out-wikitext
iter 1250: loss 5.8881, time 4811.09ms
iter 1300: loss 5.8673, time 1219.21ms
iter 1350: loss 5.6796, time 1222.82ms
iter 1400: loss 5.6848, time 1221.95ms
iter 1450: loss 5.6679, time 1220.68ms
step 1500: train loss 5.5600, val loss 5.0397
saving checkpoint to out-wikitext
iter 1500: loss 5.4324, time 6976.62ms
iter 1550: loss 5.6576, time 1224.85ms
iter 1600: loss 5.8254, time 1220.45ms
iter 1650: loss 5.4978, time 1217.16ms
iter 1700: loss 5.8606, time 1220.92ms
step 1750: train loss 5.5027, val loss 5.0926
iter 1750: loss 5.6089, time 4603.64ms
iter 1800: loss 5.3451, time 1214.08ms
iter 1850: loss 5.4481, time 1216.84ms
iter 1900: loss 5.7291, time 1213.76ms
iter 1950: loss 5.6190, time 1220.70ms
step 2000: train loss 5.3741, val loss 4.9959
saving checkpoint to out-wikitext
iter 2000: loss 5.3795, time 4816.04ms
iter 2050: loss 5.5902, time 1220.80ms
iter 2100: loss 5.8346, time 1225.03ms
iter 2150: loss 5.5335, time 1229.24ms
iter 2200: loss 5.5343, time 1231.33ms
step 2250: train loss 5.3866, val loss 4.9837
saving checkpoint to out-wikitext
iter 2250: loss 5.5180, time 4897.11ms
iter 2300: loss 5.6981, time 1219.65ms
iter 2350: loss 5.6097, time 1222.66ms
iter 2400: loss 5.7352, time 1225.75ms
iter 2450: loss 5.3848, time 1209.27ms
step 2500: train loss 5.2889, val loss 4.9053
saving checkpoint to out-wikitext
iter 2500: loss 5.8564, time 7002.88ms
iter 2550: loss 5.2139, time 1205.70ms
iter 2600: loss 5.7260, time 1218.93ms
iter 2650: loss 5.3385, time 1227.33ms
iter 2700: loss 5.3243, time 1225.49ms
step 2750: train loss 5.2399, val loss 4.9387
iter 2750: loss 5.0703, time 4599.22ms
iter 2800: loss 5.3945, time 1214.75ms
iter 2850: loss 5.5997, time 1209.69ms
iter 2900: loss 5.4282, time 1215.52ms
iter 2950: loss 5.1102, time 1230.31ms
step 3000: train loss 5.3307, val loss 4.8578
saving checkpoint to out-wikitext
iter 3000: loss 5.1339, time 4798.41ms
iter 3050: loss 5.2256, time 1237.62ms
iter 3100: loss 5.4192, time 1219.38ms
iter 3150: loss 5.6915, time 1224.14ms
iter 3200: loss 5.4067, time 1207.36ms
step 3250: train loss 5.1787, val loss 4.8603
iter 3250: loss 5.4293, time 4617.33ms
iter 3300: loss 5.3937, time 1227.65ms
iter 3350: loss 5.2987, time 1222.04ms
iter 3400: loss 5.5415, time 1219.18ms
iter 3450: loss 5.2188, time 1227.77ms
step 3500: train loss 5.2286, val loss 4.8416
saving checkpoint to out-wikitext
iter 3500: loss 5.4111, time 4803.81ms
iter 3550: loss 5.5218, time 1231.24ms
iter 3600: loss 5.4410, time 1220.51ms
iter 3650: loss 5.2055, time 1219.85ms
iter 3700: loss 5.3022, time 1226.24ms
step 3750: train loss 5.2141, val loss 4.8068
saving checkpoint to out-wikitext
iter 3750: loss 5.5381, time 4963.17ms
iter 3800: loss 5.1362, time 1219.30ms
iter 3850: loss 4.8173, time 1214.75ms
iter 3900: loss 5.1167, time 1215.02ms
iter 3950: loss 5.2159, time 1230.70ms
step 4000: train loss 5.1490, val loss 4.9066
iter 4000: loss 5.3482, time 4586.36ms
iter 4050: loss 5.1113, time 1216.16ms
iter 4100: loss 5.5044, time 1212.31ms
iter 4150: loss 5.1989, time 1208.13ms
iter 4200: loss 5.2133, time 1219.34ms
step 4250: train loss 5.0911, val loss 4.8469
iter 4250: loss 5.2981, time 4636.09ms
iter 4300: loss 5.3183, time 1214.68ms
iter 4350: loss 5.3617, time 1227.46ms
iter 4400: loss 4.7645, time 1215.14ms
iter 4450: loss 5.0698, time 1210.71ms
step 4500: train loss 5.0551, val loss 4.8802
iter 4500: loss 4.9698, time 4783.27ms
iter 4550: loss 5.1656, time 1265.87ms
iter 4600: loss 4.9590, time 1229.97ms
iter 4650: loss 5.1512, time 1226.13ms
iter 4700: loss 5.4552, time 1215.14ms
step 4750: train loss 5.0841, val loss 4.7097
saving checkpoint to out-wikitext
iter 4750: loss 5.3861, time 4829.25ms
iter 4800: loss 5.3392, time 1227.21ms
iter 4850: loss 4.8760, time 1218.56ms
iter 4900: loss 4.9675, time 1223.43ms
iter 4950: loss 5.1812, time 1216.79ms
step 5000: train loss 4.9741, val loss 4.8620
iter 5000: loss 5.1709, time 4613.21ms
iter 5050: loss 4.8640, time 1233.02ms
iter 5100: loss 5.0871, time 1226.78ms
iter 5150: loss 5.0683, time 1222.79ms
iter 5200: loss 4.9645, time 1228.27ms
step 5250: train loss 5.0272, val loss 4.7513
iter 5250: loss 4.8501, time 4620.80ms
iter 5300: loss 5.0726, time 1226.77ms
iter 5350: loss 5.4350, time 1231.08ms
iter 5400: loss 5.0934, time 1232.90ms
iter 5450: loss 5.3295, time 1224.31ms
step 5500: train loss 5.0830, val loss 4.8136
iter 5500: loss 4.8329, time 4602.90ms
iter 5550: loss 5.0271, time 1251.85ms
iter 5600: loss 4.9304, time 1222.72ms
iter 5650: loss 5.1215, time 1228.61ms
iter 5700: loss 5.6325, time 1208.75ms
step 5750: train loss 4.9741, val loss 4.7212
iter 5750: loss 5.5023, time 4695.53ms
iter 5800: loss 5.0054, time 1320.77ms
iter 5850: loss 5.3559, time 1249.86ms
iter 5900: loss 5.1498, time 1292.40ms
iter 5950: loss 5.0790, time 1254.05ms
step 6000: train loss 5.0083, val loss 4.8332
iter 6000: loss 4.8881, time 4632.44ms
iter 6050: loss 5.1868, time 1235.60ms
iter 6100: loss 5.3753, time 1220.08ms
iter 6150: loss 5.2215, time 1222.20ms
iter 6200: loss 5.2344, time 1215.56ms
step 6250: train loss 4.9323, val loss 4.7253
iter 6250: loss 5.3742, time 4986.58ms
iter 6300: loss 5.1076, time 1237.13ms
iter 6350: loss 5.0150, time 1221.17ms
iter 6400: loss 5.2793, time 1221.31ms
iter 6450: loss 5.1101, time 1252.54ms
step 6500: train loss 4.9252, val loss 4.7494
iter 6500: loss 5.2840, time 4630.04ms
iter 6550: loss 5.1892, time 1225.53ms
iter 6600: loss 5.0398, time 1225.75ms
iter 6650: loss 5.1881, time 1219.22ms
iter 6700: loss 5.0668, time 1220.67ms
step 6750: train loss 4.9108, val loss 4.8187
iter 6750: loss 4.7885, time 4590.79ms
iter 6800: loss 5.0995, time 1222.72ms
iter 6850: loss 4.8694, time 1228.26ms
iter 6900: loss 5.0110, time 1213.70ms
iter 6950: loss 5.1870, time 1214.30ms
step 7000: train loss 4.8794, val loss 4.7565
iter 7000: loss 5.2052, time 4635.96ms
iter 7050: loss 5.1447, time 1215.26ms
iter 7100: loss 5.1906, time 1222.30ms
iter 7150: loss 5.0133, time 1214.22ms
iter 7200: loss 4.8762, time 1222.54ms
step 7250: train loss 4.8608, val loss 4.7410
iter 7250: loss 5.6640, time 4618.37ms
iter 7300: loss 4.9968, time 1229.57ms
iter 7350: loss 4.8109, time 1213.16ms
iter 7400: loss 5.1364, time 1211.88ms
iter 7450: loss 4.8608, time 1224.59ms
step 7500: train loss 4.8512, val loss 4.7080
saving checkpoint to out-wikitext
iter 7500: loss 5.0417, time 7237.42ms
iter 7550: loss 5.4485, time 1207.63ms
iter 7600: loss 5.1332, time 1222.25ms
iter 7650: loss 5.1018, time 1216.40ms
iter 7700: loss 5.0751, time 1211.50ms
step 7750: train loss 4.9277, val loss 4.6410
saving checkpoint to out-wikitext
iter 7750: loss 5.3768, time 6974.27ms
iter 7800: loss 4.9130, time 1220.72ms
iter 7850: loss 5.2906, time 1220.09ms
iter 7900: loss 5.2913, time 1213.83ms
iter 7950: loss 5.1144, time 1225.08ms
step 8000: train loss 4.7866, val loss 4.6833
iter 8000: loss 4.7817, time 4620.00ms
iter 8050: loss 4.8350, time 1214.69ms
iter 8100: loss 5.2343, time 1226.25ms
iter 8150: loss 5.2125, time 1219.90ms
iter 8200: loss 4.7241, time 1216.03ms
step 8250: train loss 4.8297, val loss 4.7418
iter 8250: loss 5.0728, time 4629.00ms
iter 8300: loss 5.1600, time 1219.60ms
iter 8350: loss 5.0669, time 1223.57ms
iter 8400: loss 5.0245, time 1220.93ms
iter 8450: loss 5.1521, time 1220.57ms
step 8500: train loss 4.7549, val loss 4.7242
iter 8500: loss 4.9963, time 4628.86ms
iter 8550: loss 4.9207, time 1225.95ms
iter 8600: loss 5.1352, time 1223.18ms
iter 8650: loss 4.7157, time 1215.17ms
iter 8700: loss 4.8103, time 1228.39ms
step 8750: train loss 4.9214, val loss 4.6794
iter 8750: loss 4.7218, time 4620.56ms
iter 8800: loss 5.1922, time 1217.15ms
iter 8850: loss 4.9514, time 1227.21ms
iter 8900: loss 4.8425, time 1217.94ms
iter 8950: loss 5.0347, time 1212.13ms
step 9000: train loss 4.8904, val loss 4.7019
iter 9000: loss 5.0449, time 4621.53ms
iter 9050: loss 5.0745, time 1207.49ms
iter 9100: loss 4.9424, time 1215.44ms
iter 9150: loss 5.0588, time 1218.98ms
iter 9200: loss 5.4345, time 1225.09ms
step 9250: train loss 4.8268, val loss 4.6784
iter 9250: loss 4.4053, time 4609.30ms
iter 9300: loss 4.8528, time 1209.49ms
iter 9350: loss 4.8809, time 1217.52ms
iter 9400: loss 5.0715, time 1217.83ms
iter 9450: loss 4.8531, time 1212.58ms
step 9500: train loss 4.8614, val loss 4.6722
iter 9500: loss 5.1817, time 4658.48ms
iter 9550: loss 5.1955, time 1220.75ms
iter 9600: loss 5.2271, time 1224.28ms
iter 9650: loss 4.7655, time 1209.77ms
iter 9700: loss 4.7550, time 1219.54ms
step 9750: train loss 4.8622, val loss 4.7855
iter 9750: loss 4.8639, time 4612.03ms
iter 9800: loss 4.9074, time 1218.74ms
iter 9850: loss 5.3559, time 1214.53ms
iter 9900: loss 5.0196, time 1229.20ms
iter 9950: loss 4.9897, time 1214.85ms
step 10000: train loss 4.8178, val loss 4.6270
saving checkpoint to out-wikitext
iter 10000: loss 4.8249, time 4827.67ms
iter 10050: loss 4.8709, time 1216.21ms
iter 10100: loss 5.1024, time 1226.97ms
iter 10150: loss 4.9456, time 1219.59ms
iter 10200: loss 5.0724, time 1217.73ms
step 10250: train loss 4.8322, val loss 4.6645
iter 10250: loss 4.9160, time 4621.05ms
iter 10300: loss 4.8726, time 1219.44ms
iter 10350: loss 5.1808, time 1218.61ms
iter 10400: loss 5.2974, time 1207.74ms
iter 10450: loss 5.0043, time 1210.38ms
step 10500: train loss 4.7783, val loss 4.7618
iter 10500: loss 5.2081, time 4649.89ms
iter 10550: loss 5.1386, time 1218.45ms
iter 10600: loss 4.8900, time 1223.38ms
iter 10650: loss 4.7399, time 1222.40ms
iter 10700: loss 5.0684, time 1232.68ms
step 10750: train loss 4.8110, val loss 4.7174
iter 10750: loss 4.7767, time 4617.45ms
iter 10800: loss 5.1021, time 1229.61ms
iter 10850: loss 5.2615, time 1222.68ms
iter 10900: loss 5.0410, time 1228.39ms
iter 10950: loss 4.7928, time 1208.21ms
step 11000: train loss 4.7553, val loss 4.6378
iter 11000: loss 5.1263, time 4633.65ms
iter 11050: loss 5.1882, time 1224.08ms
iter 11100: loss 5.0129, time 1223.84ms
iter 11150: loss 4.9708, time 1219.12ms
iter 11200: loss 4.6501, time 1221.68ms
step 11250: train loss 4.7356, val loss 4.6587
iter 11250: loss 5.0067, time 4595.12ms
iter 11300: loss 4.6173, time 1236.00ms
iter 11350: loss 4.8167, time 1225.16ms
iter 11400: loss 4.8384, time 1216.64ms
iter 11450: loss 5.0446, time 1207.45ms
step 11500: train loss 4.7513, val loss 4.7261
iter 11500: loss 4.9181, time 4610.71ms
iter 11550: loss 4.9190, time 1227.08ms
iter 11600: loss 5.2439, time 1218.97ms
iter 11650: loss 4.9601, time 1233.76ms
iter 11700: loss 5.0815, time 1216.53ms
step 11750: train loss 4.7468, val loss 4.7072
iter 11750: loss 4.8176, time 4614.69ms
iter 11800: loss 4.8591, time 1218.07ms
iter 11850: loss 4.7082, time 1214.98ms
iter 11900: loss 4.7134, time 1222.20ms
iter 11950: loss 5.3583, time 1210.64ms
step 12000: train loss 4.7530, val loss 4.6796
iter 12000: loss 5.1056, time 4628.23ms
iter 12050: loss 4.9157, time 1221.62ms
iter 12100: loss 4.5154, time 1222.57ms
iter 12150: loss 4.7941, time 1205.91ms
iter 12200: loss 4.9001, time 1226.60ms
step 12250: train loss 4.7734, val loss 4.6252
saving checkpoint to out-wikitext
iter 12250: loss 4.8246, time 4811.31ms
iter 12300: loss 5.0559, time 1219.55ms
iter 12350: loss 4.9065, time 1224.08ms
iter 12400: loss 5.1921, time 1222.02ms
iter 12450: loss 4.6612, time 1215.58ms
step 12500: train loss 4.8144, val loss 4.6556
iter 12500: loss 5.0662, time 4626.11ms
iter 12550: loss 4.9700, time 1228.30ms
iter 12600: loss 4.6329, time 1217.68ms
iter 12650: loss 4.7604, time 1219.70ms
iter 12700: loss 5.2670, time 1223.10ms
step 12750: train loss 4.7442, val loss 4.6654
iter 12750: loss 5.0560, time 4631.60ms
iter 12800: loss 4.9286, time 1221.78ms
iter 12850: loss 5.0595, time 1220.23ms
iter 12900: loss 4.8985, time 1226.15ms
iter 12950: loss 5.1115, time 1224.72ms
step 13000: train loss 4.7386, val loss 4.6363
iter 13000: loss 4.9260, time 4615.24ms
iter 13050: loss 4.7081, time 1222.12ms
iter 13100: loss 4.6746, time 1219.97ms
iter 13150: loss 5.0097, time 1223.33ms
iter 13200: loss 5.2153, time 1223.27ms
step 13250: train loss 4.6856, val loss 4.6813
iter 13250: loss 4.9554, time 4615.29ms
iter 13300: loss 5.1075, time 1221.37ms
iter 13350: loss 4.9388, time 1230.40ms
iter 13400: loss 4.7477, time 1221.56ms
iter 13450: loss 4.8361, time 1224.80ms
step 13500: train loss 4.6780, val loss 4.6183
saving checkpoint to out-wikitext
iter 13500: loss 4.9714, time 4819.18ms
iter 13550: loss 5.1359, time 1210.03ms
iter 13600: loss 4.8841, time 1221.39ms
iter 13650: loss 5.3587, time 1217.65ms
iter 13700: loss 5.2477, time 1223.78ms
step 13750: train loss 4.6605, val loss 4.6173
saving checkpoint to out-wikitext
iter 13750: loss 4.7567, time 4791.05ms
iter 13800: loss 4.9736, time 1224.10ms
iter 13850: loss 4.5207, time 1222.40ms
iter 13900: loss 4.7327, time 1226.03ms
iter 13950: loss 4.6102, time 1223.69ms
step 14000: train loss 4.6757, val loss 4.6197
iter 14000: loss 4.9578, time 4616.28ms
iter 14050: loss 5.2617, time 1222.63ms
iter 14100: loss 4.6206, time 1224.92ms
iter 14150: loss 4.7828, time 1221.14ms
iter 14200: loss 4.8925, time 1224.40ms
step 14250: train loss 4.7462, val loss 4.6763
iter 14250: loss 4.8634, time 4616.82ms
iter 14300: loss 5.0250, time 1221.73ms
iter 14350: loss 4.8197, time 1216.04ms
iter 14400: loss 4.8401, time 1216.22ms
iter 14450: loss 4.9537, time 1215.53ms
step 14500: train loss 4.7096, val loss 4.5672
saving checkpoint to out-wikitext
iter 14500: loss 5.0806, time 4779.91ms
iter 14550: loss 4.9676, time 1212.09ms
iter 14600: loss 4.7862, time 1210.54ms
iter 14650: loss 4.8876, time 1217.17ms
iter 14700: loss 4.7380, time 1209.09ms
step 14750: train loss 4.6128, val loss 4.5401
saving checkpoint to out-wikitext
iter 14750: loss 5.1160, time 4789.86ms
iter 14800: loss 4.6786, time 1213.53ms
iter 14850: loss 4.4908, time 1216.19ms
iter 14900: loss 5.1050, time 1212.99ms
iter 14950: loss 5.3573, time 1221.11ms
step 15000: train loss 4.6762, val loss 4.7253
iter 15000: loss 4.7476, time 4598.17ms
iter 15050: loss 4.7226, time 1217.09ms
iter 15100: loss 4.5832, time 1223.66ms
iter 15150: loss 4.8261, time 1216.72ms
iter 15200: loss 4.7042, time 1221.53ms
step 15250: train loss 4.6380, val loss 4.5330
saving checkpoint to out-wikitext
iter 15250: loss 4.9691, time 4786.09ms
iter 15300: loss 4.9408, time 1215.44ms
iter 15350: loss 4.6421, time 1224.97ms
iter 15400: loss 4.9928, time 1202.13ms
iter 15450: loss 4.8074, time 1221.43ms
step 15500: train loss 4.7211, val loss 4.4984
saving checkpoint to out-wikitext
iter 15500: loss 4.9973, time 4798.84ms
iter 15550: loss 4.9423, time 1226.62ms
iter 15600: loss 4.9450, time 1225.96ms
iter 15650: loss 4.6850, time 1221.71ms
iter 15700: loss 5.2323, time 1234.01ms
step 15750: train loss 4.6584, val loss 4.6230
iter 15750: loss 4.7476, time 4631.62ms
iter 15800: loss 4.9830, time 1210.69ms
iter 15850: loss 5.3276, time 1229.32ms
iter 15900: loss 5.0595, time 1211.13ms
iter 15950: loss 4.6159, time 1219.34ms
step 16000: train loss 4.5867, val loss 4.6502
iter 16000: loss 4.7896, time 4605.19ms
iter 16050: loss 4.7545, time 1223.52ms
iter 16100: loss 4.6634, time 1219.03ms
iter 16150: loss 4.8471, time 1206.37ms
iter 16200: loss 4.8533, time 1217.71ms
step 16250: train loss 4.6558, val loss 4.5668
iter 16250: loss 4.9207, time 4632.34ms
iter 16300: loss 5.0490, time 1225.47ms
iter 16350: loss 5.1259, time 1221.71ms
iter 16400: loss 4.6630, time 1220.60ms
iter 16450: loss 4.8799, time 1209.19ms
step 16500: train loss 4.6365, val loss 4.6021
iter 16500: loss 4.8893, time 4620.99ms
iter 16550: loss 4.7905, time 1219.87ms
iter 16600: loss 4.8005, time 1236.29ms
iter 16650: loss 4.8276, time 1228.30ms
iter 16700: loss 4.7076, time 1228.13ms
step 16750: train loss 4.6481, val loss 4.6579
iter 16750: loss 4.7888, time 4619.92ms
iter 16800: loss 5.0236, time 1218.12ms
iter 16850: loss 4.8649, time 1220.61ms
iter 16900: loss 4.8011, time 1218.13ms
iter 16950: loss 5.1301, time 1212.94ms
step 17000: train loss 4.6902, val loss 4.6747
iter 17000: loss 4.9676, time 4618.68ms
iter 17050: loss 4.8444, time 1217.74ms
iter 17100: loss 4.9830, time 1215.49ms
iter 17150: loss 4.6329, time 1217.68ms
iter 17200: loss 4.6694, time 1222.82ms
step 17250: train loss 4.6517, val loss 4.5632
iter 17250: loss 4.9178, time 4583.56ms
iter 17300: loss 4.7083, time 1225.05ms
iter 17350: loss 4.5982, time 1233.03ms
iter 17400: loss 4.6196, time 1217.81ms
iter 17450: loss 4.7590, time 1225.30ms
step 17500: train loss 4.6125, val loss 4.6461
iter 17500: loss 4.7132, time 4622.02ms
iter 17550: loss 4.7469, time 1220.55ms
iter 17600: loss 5.1502, time 1224.73ms
iter 17650: loss 4.7139, time 1225.54ms
iter 17700: loss 4.7064, time 1210.32ms
step 17750: train loss 4.6092, val loss 4.6976
iter 17750: loss 4.8600, time 4630.00ms
iter 17800: loss 4.8987, time 1223.26ms
iter 17850: loss 4.6195, time 1228.48ms
iter 17900: loss 4.5237, time 1222.54ms
iter 17950: loss 4.7731, time 1237.95ms
step 18000: train loss 4.6254, val loss 4.5846
iter 18000: loss 4.7976, time 4632.82ms
iter 18050: loss 4.9568, time 1211.42ms
iter 18100: loss 5.0549, time 1216.02ms
iter 18150: loss 4.8109, time 1219.68ms
iter 18200: loss 4.9755, time 1221.46ms
step 18250: train loss 4.6198, val loss 4.5829
iter 18250: loss 4.7045, time 4609.34ms
iter 18300: loss 4.8432, time 1222.20ms
iter 18350: loss 4.6198, time 1201.47ms
iter 18400: loss 5.2863, time 1232.57ms
iter 18450: loss 4.8205, time 1218.14ms
step 18500: train loss 4.6275, val loss 4.5174
iter 18500: loss 5.1059, time 4594.54ms
iter 18550: loss 4.6378, time 1216.81ms
iter 18600: loss 4.9846, time 1214.25ms
iter 18650: loss 4.7466, time 1205.15ms
iter 18700: loss 4.9349, time 1214.68ms
step 18750: train loss 4.6183, val loss 4.6598
iter 18750: loss 4.9355, time 4596.62ms
iter 18800: loss 4.9020, time 1216.97ms
iter 18850: loss 4.6876, time 1216.69ms
iter 18900: loss 4.5863, time 1214.21ms
iter 18950: loss 4.7249, time 1213.40ms
step 19000: train loss 4.5637, val loss 4.4893
saving checkpoint to out-wikitext
iter 19000: loss 4.9546, time 4859.63ms
iter 19050: loss 4.9250, time 1215.82ms
iter 19100: loss 4.8127, time 1214.90ms
iter 19150: loss 4.8393, time 1225.57ms
iter 19200: loss 4.7381, time 1218.52ms
step 19250: train loss 4.5619, val loss 4.6781
iter 19250: loss 5.2245, time 4591.04ms
iter 19300: loss 4.8036, time 1228.37ms
iter 19350: loss 4.8356, time 1230.51ms
iter 19400: loss 4.9449, time 1225.08ms
iter 19450: loss 5.1211, time 1224.01ms
step 19500: train loss 4.6020, val loss 4.5620
iter 19500: loss 4.8801, time 4635.23ms
iter 19550: loss 4.9376, time 1222.96ms
iter 19600: loss 5.1701, time 1219.79ms
iter 19650: loss 4.9660, time 1222.61ms
iter 19700: loss 5.0707, time 1218.15ms
step 19750: train loss 4.6419, val loss 4.7170
iter 19750: loss 5.1389, time 4602.95ms
iter 19800: loss 4.9027, time 1223.28ms
iter 19850: loss 4.8196, time 1224.02ms
iter 19900: loss 4.9911, time 1218.46ms
iter 19950: loss 4.9509, time 1216.29ms
step 20000: train loss 4.6238, val loss 4.6588
iter 20000: loss 4.5823, time 4592.08ms
